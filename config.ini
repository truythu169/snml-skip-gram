[PREPROCESS]
output_data = data.csv
output_dict_path = dict/
vocab_to_int = vocab_to_int.dict
int_to_vocab = int_to_vocab.dict
cont_to_int = cont_to_int.dict
int_to_cont = int_to_cont.dict
threshold = 1e-5
n_top = 30000

[TRAIN]
train_data = 2/train1.csv
train_data_snml = data_snml.csv
scope = scope.csv
vocab_dict = dict/int_to_vocab.dict
int_vocab_dict = dict/vocab_to_int.dict
context_dict = dict/int_to_cont.dict
question_file = ../evaluation/datasets/word_analogy/google_analogy.txt
top_word_file = ../../data/text8/top_30000_words.txt

output_dir = {}dim/
embedding = embedding-e={}-n_sampled={}-epochs={}-batch_size={}.txt
embedding_pkl = embedding.pkl
softmax_w_pkl = softmax_w.pkl
softmax_b_pkl = softmax_b.pkl
loss_file = loss.pkl
acc_file = acc.pkl

[SNML]
n_files = 12802
embedding = embedding.pkl
softmax_w = softmax_w.pkl
softmax_b = softmax_b.pkl
context_dis = ../
context_sam = ../

[SETTING]
seed = 2509
