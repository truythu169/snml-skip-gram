{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "import os, csv, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "n_context = 50\n",
    "n_data = 50000\n",
    "n_pairs = 10\n",
    "noise_rate = 0.2\n",
    "filename = '../evaluation/datasets/word_analogy/google_analogy.txt'\n",
    "filepath = '../evaluation/datasets/word_analogy/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_word_analogy(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        L = f.read().splitlines()\n",
    "\n",
    "    questions = []\n",
    "    for l in L:\n",
    "        l = l.lower()\n",
    "        if not l.startswith(\":\"):\n",
    "            words = l.split()\n",
    "            questions.append(words)\n",
    "            \n",
    "    return questions\n",
    "\n",
    "def sample_questions(filename, pair_size):\n",
    "    questions = read_word_analogy(filename)\n",
    "    \n",
    "    # Random question pairs\n",
    "    # Exclude pairs such as : (w1, w2), (w1, w3)\n",
    "    pairs = {}\n",
    "    words = set()\n",
    "    while True:\n",
    "        idx = random.randrange(len(questions))\n",
    "        new_word = True\n",
    "        for word in questions[idx]:\n",
    "            if word in words:\n",
    "                new_word = False\n",
    "                break\n",
    "        \n",
    "        if new_word:\n",
    "            word1 = questions[idx][0]\n",
    "            word2 = questions[idx][1]\n",
    "            word3 = questions[idx][2]\n",
    "            word4 = questions[idx][3]\n",
    "            pair1 = make_pair(word1, word2)\n",
    "            pair2 = make_pair(word3, word4)\n",
    "            \n",
    "            if pair1 not in pairs and pair2 not in pairs:\n",
    "                words.add(word1)\n",
    "                words.add(word2)\n",
    "                words.add(word3)\n",
    "                words.add(word4)\n",
    "                pairs[pair1] = len(pairs)\n",
    "                pairs[pair2] = len(pairs)\n",
    "            \n",
    "        if len(pairs) >= pair_size:\n",
    "            break\n",
    "    \n",
    "    # Generate questions\n",
    "    output = []\n",
    "    for question in questions:\n",
    "        pair1 = make_pair(question[0], question[1])\n",
    "        pair2 = make_pair(question[2], question[3])\n",
    "        \n",
    "        if pair1 in pairs and pair2 in pairs:\n",
    "            output.append(question)\n",
    "            # print questions\n",
    "#             print('{} {} {} {}'.format(question[0], question[1], question[2], question[3]))\n",
    "        \n",
    "    return output, pairs\n",
    "\n",
    "def make_pair(word1, word2):\n",
    "    if word1 < word2:\n",
    "        return (word1, word2)\n",
    "    else:\n",
    "        return (word2, word1)\n",
    "\n",
    "def initialize_dict(sample_set):\n",
    "    words = set()\n",
    "    for question in sample_set:\n",
    "        for word in question:\n",
    "            words.add(word)\n",
    "    \n",
    "    int_to_word = {ii: word for ii, word in enumerate(words)}\n",
    "    word_to_int = {word: ii for ii, word in int_to_word.items()}\n",
    "    \n",
    "    return int_to_word, word_to_int\n",
    "\n",
    "def question_to_int(questions, word_to_int):\n",
    "    int_question = [[word_to_int[word] for word in question] for question in questions]\n",
    "    return int_question\n",
    "\n",
    "def cosine(vec1, vec2):\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "\n",
    "    return vec1.dot(vec2) / (norm1 * norm2)\n",
    "\n",
    "def save_pkl(data, filename, local=False):\n",
    "    \"\"\" Save data to file \"\"\"\n",
    "    # create path\n",
    "    parent_dir = os.path.dirname(filename)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "\n",
    "    # save file\n",
    "    output = open(filename, 'wb')\n",
    "    pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)\n",
    "    output.close()\n",
    "    \n",
    "def generate_word_level(questions):\n",
    "    same_level_word = []\n",
    "    word_dis_dict = {}\n",
    "    for question in questions:\n",
    "        for i in range(2):\n",
    "            if question[i] in word_dis_dict and question[i+2] in word_dis_dict:\n",
    "                if word_dis_dict[question[i]] != word_dis_dict[question[i+2]]:\n",
    "                    # merge two level\n",
    "                    merge_idx = word_dis_dict[question[i]]\n",
    "                    level2 = same_level_word[word_dis_dict[question[i+2]]]\n",
    "                    level = same_level_word[merge_idx]|level2\n",
    "                    same_level_word[merge_idx] = level\n",
    "                    for word in level2:\n",
    "                        word_dis_dict[word] = merge_idx\n",
    "                    continue\n",
    "\n",
    "            if question[i] in word_dis_dict:\n",
    "                idx = word_dis_dict[question[i]]\n",
    "                level = same_level_word[idx]\n",
    "                same_level_word[idx].add(question[i+2])\n",
    "                \n",
    "            elif question[i+2] in word_dis_dict:\n",
    "                idx = word_dis_dict[question[i+2]]\n",
    "                level = same_level_word[idx]\n",
    "                same_level_word[idx].add(question[i])\n",
    "                \n",
    "            else:\n",
    "                idx = len(same_level_word)\n",
    "                level = {question[i], question[i+2]}\n",
    "                same_level_word.append(level)\n",
    "            \n",
    "            word_dis_dict[question[i]] = word_dis_dict[question[i+2]] = idx\n",
    "            \n",
    "    return same_level_word, word_dis_dict\n",
    "\n",
    "def generate_original_distribution(questions, context_size):\n",
    "    same_level_word, word_dis_dict = generate_word_level(questions)\n",
    "    original_distribution = []\n",
    "    for i in range(len(same_level_word)):\n",
    "        dis = np.random.rand(context_size)\n",
    "        original_distribution.append(dis)\n",
    "    \n",
    "    return original_distribution, word_dis_dict\n",
    "\n",
    "def generate_noise_distribution(n_pairs, context_size):\n",
    "    noise_distribution = []\n",
    "    for i in range(n_pairs):\n",
    "        dis = np.random.rand(context_size)\n",
    "        noise_distribution.append(dis)\n",
    "    \n",
    "    return noise_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiialize\n",
    "questions, pairs = sample_questions(filename, n_pairs)\n",
    "int_to_word, word_to_int = initialize_dict(questions)\n",
    "questions = question_to_int(questions, word_to_int)\n",
    "n_question = len(questions)\n",
    "\n",
    "# Change new parameters\n",
    "n_word = len(int_to_word)\n",
    "output_dict = 'output/{}-context-{}-data-{}-questions/'.format(n_context, n_data, n_question)\n",
    "\n",
    "# Initialize dummy contexts\n",
    "int_to_cont = {context: context for context in range(n_context)}\n",
    "cont_to_int = {word: ii for ii, word in int_to_cont.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample context distribution for each words\n",
    "original_distribution, word_dis_dict = generate_original_distribution(questions, n_context)\n",
    "noise_distribution = generate_noise_distribution(n_pairs, n_context)\n",
    "context_distribution = {}\n",
    "\n",
    "for pair, idx in pairs.items():\n",
    "    word1, word2 = pair\n",
    "    word1 = word_to_int[word1]\n",
    "    word2 = word_to_int[word2]\n",
    "    \n",
    "    # word1\n",
    "    dis = (1- noise_rate) * original_distribution[word_dis_dict[word1]].copy() + noise_rate * noise_distribution[idx]\n",
    "    dis = dis / np.sum(dis)\n",
    "    context_distribution[word1] = dis\n",
    "    \n",
    "    # word2\n",
    "    dis = (1- noise_rate) * original_distribution[word_dis_dict[word2]].copy() + noise_rate * noise_distribution[idx]\n",
    "    dis = dis / np.sum(dis)\n",
    "    context_distribution[word2] = dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1 antananarivo-madagascar: 0.8229622451983442, Pair 2 dushanbe-tajikistan: 0.8182251706647685, diff: 0.004737074533575636\n",
      "Pair 1 dushanbe-tajikistan: 0.8182251706647685, Pair 2 minsk-belarus: 0.8132208179929188, diff: 0.005004352671849732\n",
      "Pair 1 minsk-belarus: 0.8132208179929188, Pair 2 santiago-chile: 0.8166286521664511, diff: 0.003407834173532298\n",
      "Pair 1 santiago-chile: 0.8166286521664511, Pair 2 antananarivo-madagascar: 0.8229622451983442, diff: 0.00633359303189307\n",
      "Pair 1 high-higher: 0.8596074666392032, Pair 2 safe-safer: 0.862514809411728, diff: 0.0029073427725248013\n",
      "Pair 1 safe-safer: 0.862514809411728, Pair 2 high-higher: 0.8596074666392032, diff: 0.0029073427725248013\n",
      "Pair 1 cool-coolest: 0.8371847899503484, Pair 2 short-shortest: 0.844993900174008, diff: 0.007809110223659599\n",
      "Pair 1 short-shortest: 0.844993900174008, Pair 2 cool-coolest: 0.8371847899503484, diff: 0.007809110223659599\n",
      "Pair 1 decreasing-decreased: 0.7679342785505269, Pair 2 paying-paid: 0.781020824990799, diff: 0.013086546440272073\n",
      "Pair 1 paying-paid: 0.781020824990799, Pair 2 decreasing-decreased: 0.7679342785505269, diff: 0.013086546440272073\n"
     ]
    }
   ],
   "source": [
    "# Test for word analogy property\n",
    "for i in range(n_question):\n",
    "    question = questions[i]\n",
    "    pair1_dis = cosine(context_distribution[question[0]], context_distribution[question[1]])\n",
    "    pair2_dis = cosine(context_distribution[question[2]], context_distribution[question[3]])\n",
    "    print('Pair 1 {}-{}: {}, Pair 2 {}-{}: {}, diff: {}'.format(int_to_word[question[0]], int_to_word[question[1]], pair1_dis,\n",
    "                                                                int_to_word[question[2]], int_to_word[question[3]], pair2_dis,\n",
    "                                                                abs(pair1_dis - pair2_dis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1 antananarivo-minsk: 0.9878241549835707, Pair 2 paid-paid: 1.0000000000000002, diff: 0.012175845016429543\n"
     ]
    }
   ],
   "source": [
    "# Random pairs\n",
    "pair1_dis = cosine(context_distribution[questions[0][0]], context_distribution[questions[1][2]])\n",
    "pair2_dis = cosine(context_distribution[questions[-1][1]], context_distribution[questions[-2][3]])\n",
    "print('Pair 1 {}-{}: {}, Pair 2 {}-{}: {}, diff: {}'.format(int_to_word[questions[0][0]], int_to_word[questions[1][2]], pair1_dis,\n",
    "                                                            int_to_word[questions[-1][1]], int_to_word[questions[-2][3]], pair2_dis,\n",
    "                                                            abs(pair1_dis - pair2_dis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Test word analogy score\n",
    "result = []\n",
    "for question in questions:\n",
    "    # Predict\n",
    "    answer = question[3]\n",
    "    pred = context_distribution[question[1]] - context_distribution[question[0]] + context_distribution[question[2]]\n",
    "    sim_vector = np.zeros(n_word)\n",
    "    for i in range(n_word):\n",
    "        if i in question[:3]:\n",
    "            sim_vector[i] = 0\n",
    "        else:\n",
    "            sim_vector[i] = cosine(pred, context_distribution[i])\n",
    "    pred = np.argsort(sim_vector)[-1]\n",
    "    result.append(1 if pred == answer else 0)\n",
    "\n",
    "print('Acc: ', np.mean(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training set\n",
    "data = []\n",
    "for i in range(n_data):\n",
    "    # Sample word\n",
    "    word = random.randrange(n_word)\n",
    "    \n",
    "    # Sample context\n",
    "    dis = context_distribution[word]\n",
    "    context = np.argmax(np.random.multinomial(1, dis))\n",
    "    \n",
    "    data.append([word, context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing processed data back to file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# make directories\n",
    "if not os.path.exists(output_dict):\n",
    "    os.makedirs(output_dict)\n",
    "\n",
    "# Save data\n",
    "print('Writing processed data back to file...')\n",
    "output = open(output_dict + 'data.csv', \"w\", newline='')\n",
    "writer = csv.writer(output)\n",
    "writer.writerows(data)\n",
    "output.close()\n",
    "\n",
    "# Save dictionaries\n",
    "save_pkl(int_to_word, output_dict + 'dict/int_to_vocab.dict')\n",
    "save_pkl(word_to_int, output_dict + 'dict/vocab_to_int.dict')\n",
    "save_pkl(cont_to_int, output_dict + 'dict/cont_to_int.dict')\n",
    "save_pkl(int_to_cont, output_dict + 'dict/int_to_cont.dict')\n",
    "print('Done!')\n",
    "\n",
    "# Save questions\n",
    "with open(filepath + 'test-{}-questions.txt'.format(n_question), 'w') as f:\n",
    "    f.write(': test-category\\n')\n",
    "    for i in range(n_question):\n",
    "        question = questions[i]\n",
    "        f.write('{} {} {} {}\\n'.format(int_to_word[question[0]], int_to_word[question[1]], int_to_word[question[2]], int_to_word[question[3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antananarivo-madagascar-dushanbe-tajikistan\n",
      "dushanbe-tajikistan-minsk-belarus\n",
      "minsk-belarus-santiago-chile\n",
      "santiago-chile-antananarivo-madagascar\n",
      "high-higher-safe-safer\n",
      "safe-safer-high-higher\n",
      "cool-coolest-short-shortest\n",
      "short-shortest-cool-coolest\n",
      "decreasing-decreased-paying-paid\n",
      "paying-paid-decreasing-decreased\n"
     ]
    }
   ],
   "source": [
    "# Test for word analogy property\n",
    "for i in range(n_question):\n",
    "    question = questions[i]\n",
    "    print('{}-{}-{}-{}'.format(int_to_word[question[0]], int_to_word[question[1]], int_to_word[question[2]], int_to_word[question[3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
