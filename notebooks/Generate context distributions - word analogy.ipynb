{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "import os, csv, pickle\n",
    "\n",
    "random.seed(16)\n",
    "np.random.seed(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "n_context = 50\n",
    "n_data = 1000000\n",
    "n_pairs = 10\n",
    "noise_rate = 0.3\n",
    "filename = '../evaluation/datasets/word_analogy/google_analogy.txt'\n",
    "filepath = '../evaluation/datasets/word_analogy/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_word_analogy(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        L = f.read().splitlines()\n",
    "\n",
    "    questions = []\n",
    "    for l in L:\n",
    "        l = l.lower()\n",
    "        if not l.startswith(\":\"):\n",
    "            words = l.split()\n",
    "            questions.append(words)\n",
    "            \n",
    "    return questions\n",
    "\n",
    "def sample_questions(filename, pair_size):\n",
    "    questions = read_word_analogy(filename)\n",
    "    \n",
    "    # Random question pairs\n",
    "    # Exclude pairs such as : (w1, w2), (w1, w3)\n",
    "    pairs = {}\n",
    "    words = set()\n",
    "    while True:\n",
    "        idx = random.randrange(len(questions))\n",
    "        new_word = True\n",
    "        for word in questions[idx]:\n",
    "            if word in words:\n",
    "                new_word = False\n",
    "                break\n",
    "        \n",
    "        if new_word:\n",
    "            word1 = questions[idx][0]\n",
    "            word2 = questions[idx][1]\n",
    "            word3 = questions[idx][2]\n",
    "            word4 = questions[idx][3]\n",
    "            pair1 = make_pair(word1, word2)\n",
    "            pair2 = make_pair(word3, word4)\n",
    "            \n",
    "            if pair1 not in pairs and pair2 not in pairs:\n",
    "                words.add(word1)\n",
    "                words.add(word2)\n",
    "                words.add(word3)\n",
    "                words.add(word4)\n",
    "                pairs[pair1] = len(pairs)\n",
    "                pairs[pair2] = len(pairs)\n",
    "            \n",
    "        if len(pairs) >= pair_size:\n",
    "            break\n",
    "    \n",
    "    # Generate questions\n",
    "    output = []\n",
    "    for question in questions:\n",
    "        pair1 = make_pair(question[0], question[1])\n",
    "        pair2 = make_pair(question[2], question[3])\n",
    "        \n",
    "        if pair1 in pairs and pair2 in pairs:\n",
    "            output.append(question)\n",
    "            # print questions\n",
    "#             print('{} {} {} {}'.format(question[0], question[1], question[2], question[3]))\n",
    "        \n",
    "    return output, pairs\n",
    "\n",
    "def make_pair(word1, word2):\n",
    "    if word1 < word2:\n",
    "        return (word1, word2)\n",
    "    else:\n",
    "        return (word2, word1)\n",
    "\n",
    "def initialize_dict(sample_set):\n",
    "    words = set()\n",
    "    for question in sample_set:\n",
    "        for word in question:\n",
    "            words.add(word)\n",
    "    \n",
    "    int_to_word = {ii: word for ii, word in enumerate(words)}\n",
    "    word_to_int = {word: ii for ii, word in int_to_word.items()}\n",
    "    \n",
    "    return int_to_word, word_to_int\n",
    "\n",
    "def question_to_int(questions, word_to_int):\n",
    "    int_question = [[word_to_int[word] for word in question] for question in questions]\n",
    "    return int_question\n",
    "\n",
    "def cosine(vec1, vec2):\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "\n",
    "    return vec1.dot(vec2) / (norm1 * norm2)\n",
    "\n",
    "def save_pkl(data, filename, local=False):\n",
    "    \"\"\" Save data to file \"\"\"\n",
    "    # create path\n",
    "    parent_dir = os.path.dirname(filename)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "\n",
    "    # save file\n",
    "    output = open(filename, 'wb')\n",
    "    pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)\n",
    "    output.close()\n",
    "    \n",
    "def generate_word_level(questions):\n",
    "    same_level_word = []\n",
    "    word_dis_dict = {}\n",
    "    for question in questions:\n",
    "        for i in range(2):\n",
    "            if question[i] in word_dis_dict and question[i+2] in word_dis_dict:\n",
    "                if word_dis_dict[question[i]] != word_dis_dict[question[i+2]]:\n",
    "                    # merge two level\n",
    "                    merge_idx = word_dis_dict[question[i]]\n",
    "                    level2 = same_level_word[word_dis_dict[question[i+2]]]\n",
    "                    level = same_level_word[merge_idx]|level2\n",
    "                    same_level_word[merge_idx] = level\n",
    "                    for word in level2:\n",
    "                        word_dis_dict[word] = merge_idx\n",
    "                    continue\n",
    "\n",
    "            if question[i] in word_dis_dict:\n",
    "                idx = word_dis_dict[question[i]]\n",
    "                level = same_level_word[idx]\n",
    "                same_level_word[idx].add(question[i+2])\n",
    "                \n",
    "            elif question[i+2] in word_dis_dict:\n",
    "                idx = word_dis_dict[question[i+2]]\n",
    "                level = same_level_word[idx]\n",
    "                same_level_word[idx].add(question[i])\n",
    "                \n",
    "            else:\n",
    "                idx = len(same_level_word)\n",
    "                level = {question[i], question[i+2]}\n",
    "                same_level_word.append(level)\n",
    "            \n",
    "            word_dis_dict[question[i]] = word_dis_dict[question[i+2]] = idx\n",
    "            \n",
    "    return same_level_word, word_dis_dict\n",
    "\n",
    "def generate_original_distribution(questions, context_size):\n",
    "    same_level_word, word_dis_dict = generate_word_level(questions)\n",
    "    original_distribution = []\n",
    "    for i in range(len(same_level_word)):\n",
    "        dis = np.random.rand(context_size)\n",
    "        original_distribution.append(dis)\n",
    "    \n",
    "    return original_distribution, word_dis_dict\n",
    "\n",
    "def generate_noise_distribution(n_pairs, context_size):\n",
    "    noise_distribution = []\n",
    "    for i in range(n_pairs):\n",
    "        dis = np.random.rand(context_size)\n",
    "        noise_distribution.append(dis)\n",
    "    \n",
    "    return noise_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiialize\n",
    "questions, pairs = sample_questions(filename, n_pairs)\n",
    "int_to_word, word_to_int = initialize_dict(questions)\n",
    "questions = question_to_int(questions, word_to_int)\n",
    "n_question = len(questions)\n",
    "\n",
    "# Change new parameters\n",
    "n_word = len(int_to_word)\n",
    "output_dict = 'output/{}-context-{}-data-{}-questions/'.format(n_context, n_data, n_question)\n",
    "\n",
    "# Initialize dummy contexts\n",
    "int_to_cont = {context: context for context in range(n_context)}\n",
    "cont_to_int = {word: ii for ii, word in int_to_cont.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample context distribution for each words\n",
    "original_distribution, word_dis_dict = generate_original_distribution(questions, n_context)\n",
    "noise_distribution = generate_noise_distribution(n_pairs, n_context)\n",
    "context_distribution = {}\n",
    "\n",
    "for pair, idx in pairs.items():\n",
    "    word1, word2 = pair\n",
    "    word1 = word_to_int[word1]\n",
    "    word2 = word_to_int[word2]\n",
    "    \n",
    "    # word1\n",
    "    dis = (1- noise_rate) * original_distribution[word_dis_dict[word1]].copy() + noise_rate * noise_distribution[idx]\n",
    "    dis = dis / np.sum(dis)\n",
    "    context_distribution[word1] = dis\n",
    "    \n",
    "    # word2\n",
    "    dis = (1- noise_rate) * original_distribution[word_dis_dict[word2]].copy() + noise_rate * noise_distribution[idx]\n",
    "    dis = dis / np.sum(dis)\n",
    "    context_distribution[word2] = dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1 occasional-occasionally: 0.896841902506855, Pair 2 professional-professionally: 0.8979120561066072, diff: 0.001070153599752155\n",
      "Pair 1 professional-professionally: 0.8979120561066072, Pair 2 occasional-occasionally: 0.896841902506855, diff: 0.001070153599752155\n",
      "Pair 1 loud-louder: 0.8916330412516867, Pair 2 tough-tougher: 0.8861241650930928, diff: 0.005508876158593967\n",
      "Pair 1 tough-tougher: 0.8861241650930928, Pair 2 loud-louder: 0.8916330412516867, diff: 0.005508876158593967\n",
      "Pair 1 fly-flying: 0.8615312124376431, Pair 2 move-moving: 0.8528778669958803, diff: 0.008653345441762772\n",
      "Pair 1 move-moving: 0.8528778669958803, Pair 2 fly-flying: 0.8615312124376431, diff: 0.008653345441762772\n",
      "Pair 1 argentina-argentinean: 0.87056871132285, Pair 2 denmark-danish: 0.8670927788704148, diff: 0.00347593245243516\n",
      "Pair 1 argentina-argentinean: 0.87056871132285, Pair 2 norway-norwegian: 0.8613135587998981, diff: 0.009255152522951837\n",
      "Pair 1 argentina-argentinean: 0.87056871132285, Pair 2 ukraine-ukrainian: 0.8623874240305077, diff: 0.008181287292342243\n",
      "Pair 1 denmark-danish: 0.8670927788704148, Pair 2 norway-norwegian: 0.8613135587998981, diff: 0.005779220070516677\n",
      "Pair 1 denmark-danish: 0.8670927788704148, Pair 2 ukraine-ukrainian: 0.8623874240305077, diff: 0.004705354839907083\n",
      "Pair 1 denmark-danish: 0.8670927788704148, Pair 2 argentina-argentinean: 0.87056871132285, diff: 0.00347593245243516\n",
      "Pair 1 norway-norwegian: 0.8613135587998981, Pair 2 ukraine-ukrainian: 0.8623874240305077, diff: 0.0010738652306095942\n",
      "Pair 1 norway-norwegian: 0.8613135587998981, Pair 2 argentina-argentinean: 0.87056871132285, diff: 0.009255152522951837\n",
      "Pair 1 norway-norwegian: 0.8613135587998981, Pair 2 denmark-danish: 0.8670927788704148, diff: 0.005779220070516677\n",
      "Pair 1 ukraine-ukrainian: 0.8623874240305077, Pair 2 argentina-argentinean: 0.87056871132285, diff: 0.008181287292342243\n",
      "Pair 1 ukraine-ukrainian: 0.8623874240305077, Pair 2 denmark-danish: 0.8670927788704148, diff: 0.004705354839907083\n",
      "Pair 1 ukraine-ukrainian: 0.8623874240305077, Pair 2 norway-norwegian: 0.8613135587998981, diff: 0.0010738652306095942\n"
     ]
    }
   ],
   "source": [
    "# Test for word analogy property\n",
    "for i in range(n_question):\n",
    "    question = questions[i]\n",
    "    pair1_dis = cosine(context_distribution[question[0]], context_distribution[question[1]])\n",
    "    pair2_dis = cosine(context_distribution[question[2]], context_distribution[question[3]])\n",
    "    print('Pair 1 {}-{}: {}, Pair 2 {}-{}: {}, diff: {}'.format(int_to_word[question[0]], int_to_word[question[1]], pair1_dis,\n",
    "                                                                int_to_word[question[2]], int_to_word[question[3]], pair2_dis,\n",
    "                                                                abs(pair1_dis - pair2_dis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1 occasional-occasional: 0.9999999999999999, Pair 2 ukrainian-danish: 0.9710437279408087, diff: 0.028956272059191157\n"
     ]
    }
   ],
   "source": [
    "# Random pairs\n",
    "pair1_dis = cosine(context_distribution[questions[0][0]], context_distribution[questions[1][2]])\n",
    "pair2_dis = cosine(context_distribution[questions[-1][1]], context_distribution[questions[-2][3]])\n",
    "print('Pair 1 {}-{}: {}, Pair 2 {}-{}: {}, diff: {}'.format(int_to_word[questions[0][0]], int_to_word[questions[1][2]], pair1_dis,\n",
    "                                                            int_to_word[questions[-1][1]], int_to_word[questions[-2][3]], pair2_dis,\n",
    "                                                            abs(pair1_dis - pair2_dis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Test word analogy score\n",
    "result = []\n",
    "for question in questions:\n",
    "    # Predict\n",
    "    answer = question[3]\n",
    "    pred = context_distribution[question[1]] - context_distribution[question[0]] + context_distribution[question[2]]\n",
    "    sim_vector = np.zeros(n_word)\n",
    "    for i in range(n_word):\n",
    "        if i in question[:3]:\n",
    "            sim_vector[i] = 0\n",
    "        else:\n",
    "            sim_vector[i] = cosine(pred, context_distribution[i])\n",
    "    pred = np.argsort(sim_vector)[-1]\n",
    "    result.append(1 if pred == answer else 0)\n",
    "    \n",
    "#     print(pred, sim_vector)\n",
    "\n",
    "print('Acc: ', np.mean(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training set\n",
    "data = []\n",
    "for i in range(n_data):\n",
    "    # Sample word\n",
    "    word = random.randrange(n_word)\n",
    "    \n",
    "    # Sample context\n",
    "    dis = context_distribution[word]\n",
    "    context = np.argmax(np.random.multinomial(1, dis))\n",
    "    \n",
    "    data.append([word, context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing processed data back to file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# make directories\n",
    "if not os.path.exists(output_dict):\n",
    "    os.makedirs(output_dict)\n",
    "\n",
    "# Save data\n",
    "print('Writing processed data back to file...')\n",
    "output = open(output_dict + 'data.csv', \"w\", newline='')\n",
    "writer = csv.writer(output)\n",
    "writer.writerows(data)\n",
    "output.close()\n",
    "\n",
    "# Save dictionaries\n",
    "save_pkl(int_to_word, output_dict + 'dict/int_to_vocab.dict')\n",
    "save_pkl(word_to_int, output_dict + 'dict/vocab_to_int.dict')\n",
    "save_pkl(cont_to_int, output_dict + 'dict/cont_to_int.dict')\n",
    "save_pkl(int_to_cont, output_dict + 'dict/int_to_cont.dict')\n",
    "\n",
    "# Save distribution\n",
    "save_pkl(context_distribution, output_dict + 'context_distribution.dict')\n",
    "\n",
    "# Save questions\n",
    "with open(filepath + 'test-{}-questions.txt'.format(n_question), 'w') as f:\n",
    "    f.write(': test-category\\n')\n",
    "    for i in range(n_question):\n",
    "        question = questions[i]\n",
    "        f.write('{} {} {} {}\\n'.format(int_to_word[question[0]], int_to_word[question[1]], int_to_word[question[2]], int_to_word[question[3]]))\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occasional-occasionally-professional-professionally\n",
      "professional-professionally-occasional-occasionally\n",
      "loud-louder-tough-tougher\n",
      "tough-tougher-loud-louder\n",
      "fly-flying-move-moving\n",
      "move-moving-fly-flying\n",
      "argentina-argentinean-denmark-danish\n",
      "argentina-argentinean-norway-norwegian\n",
      "argentina-argentinean-ukraine-ukrainian\n",
      "denmark-danish-norway-norwegian\n",
      "denmark-danish-ukraine-ukrainian\n",
      "denmark-danish-argentina-argentinean\n",
      "norway-norwegian-ukraine-ukrainian\n",
      "norway-norwegian-argentina-argentinean\n",
      "norway-norwegian-denmark-danish\n",
      "ukraine-ukrainian-argentina-argentinean\n",
      "ukraine-ukrainian-denmark-danish\n",
      "ukraine-ukrainian-norway-norwegian\n"
     ]
    }
   ],
   "source": [
    "# Test for word analogy property\n",
    "for i in range(n_question):\n",
    "    question = questions[i]\n",
    "    print('{}-{}-{}-{}'.format(int_to_word[question[0]], int_to_word[question[1]], int_to_word[question[2]], int_to_word[question[3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
