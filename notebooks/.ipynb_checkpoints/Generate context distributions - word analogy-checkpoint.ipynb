{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "import os, csv, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "n_context = 100\n",
    "n_data = 10000\n",
    "n_pairs = 100\n",
    "noise_rate = 0.05\n",
    "filename = '../evaluation/datasets/word_analogy/google_analogy.txt'\n",
    "filepath = '../evaluation/datasets/word_analogy/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_word_analogy(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        L = f.read().splitlines()\n",
    "\n",
    "    questions = []\n",
    "    for l in L:\n",
    "        l = l.lower()\n",
    "        if not l.startswith(\":\"):\n",
    "            words = l.split()\n",
    "            questions.append(words)\n",
    "            \n",
    "    return questions\n",
    "\n",
    "def sample_questions(filename, pair_size):\n",
    "    questions = read_word_analogy(filename)\n",
    "    \n",
    "    # Random question pairs\n",
    "    # Exclude pairs such as : (w1, w2), (w1, w3)\n",
    "    pairs = {}\n",
    "    words = set()\n",
    "    while True:\n",
    "        idx = random.randrange(len(questions))\n",
    "        new_word = True\n",
    "        for word in questions[idx]:\n",
    "            if word in words:\n",
    "                new_word = False\n",
    "                break\n",
    "        \n",
    "        if new_word:\n",
    "            word1 = questions[idx][0]\n",
    "            word2 = questions[idx][1]\n",
    "            word3 = questions[idx][2]\n",
    "            word4 = questions[idx][3]\n",
    "            pair1 = make_pair(word1, word2)\n",
    "            pair2 = make_pair(word3, word4)\n",
    "            \n",
    "            if pair1 not in pairs and pair2 not in pairs:\n",
    "                words.add(word1)\n",
    "                words.add(word2)\n",
    "                words.add(word3)\n",
    "                words.add(word4)\n",
    "                pairs[pair1] = len(pairs)\n",
    "                pairs[pair2] = len(pairs)\n",
    "            \n",
    "        if len(pairs) >= pair_size:\n",
    "            break\n",
    "    \n",
    "    # Generate questions\n",
    "    output = []\n",
    "    print(pairs)\n",
    "    for question in questions:\n",
    "        pair1 = make_pair(question[0], question[1])\n",
    "        pair2 = make_pair(question[2], question[3])\n",
    "        \n",
    "        if pair1 in pairs and pair2 in pairs:\n",
    "            output.append(question)\n",
    "            # print questions\n",
    "#             print('{} {} {} {}'.format(question[0], question[1], question[2], question[3]))\n",
    "        \n",
    "    return output, pairs\n",
    "\n",
    "def make_pair(word1, word2):\n",
    "    if word1 < word2:\n",
    "        return (word1, word2)\n",
    "    else:\n",
    "        return (word2, word1)\n",
    "\n",
    "def initialize_dict(sample_set):\n",
    "    words = set()\n",
    "    for question in sample_set:\n",
    "        for word in question:\n",
    "            words.add(word)\n",
    "    \n",
    "    int_to_word = {ii: word for ii, word in enumerate(words)}\n",
    "    word_to_int = {word: ii for ii, word in int_to_word.items()}\n",
    "    \n",
    "    return int_to_word, word_to_int\n",
    "\n",
    "def question_to_int(questions, word_to_int):\n",
    "    int_question = [[word_to_int[word] for word in question] for question in questions]\n",
    "    return int_question\n",
    "\n",
    "def cosine(vec1, vec2):\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "\n",
    "    return vec1.dot(vec2) / (norm1 * norm2)\n",
    "\n",
    "def save_pkl(data, filename, local=False):\n",
    "    \"\"\" Save data to file \"\"\"\n",
    "    # create path\n",
    "    parent_dir = os.path.dirname(filename)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "\n",
    "    # save file\n",
    "    output = open(filename, 'wb')\n",
    "    pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)\n",
    "    output.close()\n",
    "    \n",
    "def generate_word_level(questions):\n",
    "    same_level_word = []\n",
    "    word_dis_dict = {}\n",
    "    for question in questions:\n",
    "        for i in range(2):\n",
    "            if question[i] in word_dis_dict and question[i+2] in word_dis_dict:\n",
    "                if word_dis_dict[question[i]] != word_dis_dict[question[i+2]]:\n",
    "                    # merge two level\n",
    "                    merge_idx = word_dis_dict[question[i]]\n",
    "                    level2 = same_level_word[word_dis_dict[question[i+2]]]\n",
    "                    level = same_level_word[merge_idx]|level2\n",
    "                    same_level_word[merge_idx] = level\n",
    "                    for word in level2:\n",
    "                        word_dis_dict[word] = merge_idx\n",
    "                    continue\n",
    "\n",
    "            if question[i] in word_dis_dict:\n",
    "                idx = word_dis_dict[question[i]]\n",
    "                level = same_level_word[idx]\n",
    "                same_level_word[idx].add(question[i+2])\n",
    "                \n",
    "            elif question[i+2] in word_dis_dict:\n",
    "                idx = word_dis_dict[question[i+2]]\n",
    "                level = same_level_word[idx]\n",
    "                same_level_word[idx].add(question[i])\n",
    "                \n",
    "            else:\n",
    "                idx = len(same_level_word)\n",
    "                level = {question[i], question[i+2]}\n",
    "                same_level_word.append(level)\n",
    "            \n",
    "            word_dis_dict[question[i]] = word_dis_dict[question[i+2]] = idx\n",
    "            \n",
    "    return same_level_word, word_dis_dict\n",
    "\n",
    "def generate_original_distribution(questions, context_size):\n",
    "    same_level_word, word_dis_dict = generate_word_level(questions)\n",
    "    original_distribution = []\n",
    "    for i in range(len(same_level_word)):\n",
    "        dis = np.random.rand(context_size)\n",
    "        original_distribution.append(dis)\n",
    "    \n",
    "    return original_distribution, word_dis_dict\n",
    "\n",
    "def generate_noise_distribution(n_pairs, context_size):\n",
    "    noise_distribution = []\n",
    "    for i in range(n_pairs):\n",
    "        dis = np.random.rand(context_size)\n",
    "        noise_distribution.append(dis)\n",
    "    \n",
    "    return noise_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('writing', 'wrote'): 0, ('enhanced', 'enhancing'): 1, ('laredo', 'texas'): 2, ('florida', 'hialeah'): 3, ('provide', 'provides'): 4, ('listen', 'listens'): 5, ('flew', 'flying'): 6, ('selling', 'sold'): 7, ('copenhagen', 'denmark'): 8, ('lusaka', 'zambia'): 9, ('bright', 'brighter'): 10, ('weak', 'weaker'): 11, ('cheap', 'cheaper'): 12, ('easier', 'easy'): 13, ('road', 'roads'): 14, ('elephant', 'elephants'): 15, ('heavier', 'heavy'): 16, ('old', 'older'): 17, ('france', 'french'): 18, ('cambodia', 'cambodian'): 19, ('albania', 'tirana'): 20, ('algeria', 'algiers'): 21, ('father', 'mother'): 22, ('aunt', 'uncle'): 23, ('tunis', 'tunisia'): 24, ('beijing', 'china'): 25, ('canada', 'dollar'): 26, ('brazil', 'real'): 27, ('hit', 'hitting'): 28, ('sat', 'sitting'): 29, ('kathmandu', 'nepal'): 30, ('kenya', 'nairobi'): 31, ('convenient', 'inconvenient'): 32, ('certain', 'uncertain'): 33, ('jamaica', 'kingston'): 34, ('greenland', 'nuuk'): 35, ('quiet', 'quietly'): 36, ('happily', 'happy'): 37, ('libya', 'tripoli'): 38, ('austria', 'vienna'): 39, ('talk', 'talks'): 40, ('increase', 'increases'): 41, ('amman', 'jordan'): 42, ('apia', 'samoa'): 43, ('efficient', 'efficiently'): 44, ('rapid', 'rapidly'): 45, ('walked', 'walking'): 46, ('slowed', 'slowing'): 47, ('amazing', 'amazingly'): 48, ('occasional', 'occasionally'): 49, ('belgrade', 'serbia'): 50, ('damascus', 'syria'): 51, ('lisbon', 'portugal'): 52, ('mogadishu', 'somalia'): 53, ('honduras', 'tegucigalpa'): 54, ('laos', 'vientiane'): 55, ('lats', 'latvia'): 56, ('leu', 'romania'): 57, ('lion', 'lions'): 58, ('horse', 'horses'): 59, ('arizona', 'phoenix'): 60, ('colorado', 'denver'): 61, ('cold', 'coldest'): 62, ('big', 'biggest'): 63, ('grandfather', 'grandmother'): 64, ('man', 'woman'): 65, ('informed', 'uninformed'): 66, ('impossible', 'possible'): 67, ('stockholm', 'sweden'): 68, ('accra', 'ghana'): 69, ('doha', 'qatar'): 70, ('kiev', 'ukraine'): 71, ('russia', 'russian'): 72, ('peru', 'peruvian'): 73, ('daughters', 'sons'): 74, ('granddaughter', 'grandson'): 75, ('athens', 'greece'): 76, ('australia', 'canberra'): 77, ('mango', 'mangoes'): 78, ('eye', 'eyes'): 79, ('irresponsible', 'responsible'): 80, ('distasteful', 'tasteful'): 81, ('baghdad', 'iraq'): 82, ('harare', 'zimbabwe'): 83, ('hid', 'hiding'): 84, ('decreased', 'decreasing'): 85, ('paramaribo', 'suriname'): 86, ('iran', 'tehran'): 87, ('scream', 'screams'): 88, ('search', 'searches'): 89, ('professional', 'professionally'): 90, ('typical', 'typically'): 91, ('reluctant', 'reluctantly'): 92, ('serious', 'seriously'): 93, ('gabon', 'libreville'): 94, ('morocco', 'rabat'): 95, ('california', 'fontana'): 96, ('minneapolis', 'minnesota'): 97, ('apparent', 'apparently'): 98, ('swift', 'swiftly'): 99}\n",
      "athens greece baghdad iraq\n",
      "athens greece beijing china\n",
      "athens greece canberra australia\n",
      "athens greece stockholm sweden\n",
      "athens greece tehran iran\n",
      "baghdad iraq beijing china\n",
      "baghdad iraq canberra australia\n",
      "baghdad iraq stockholm sweden\n",
      "baghdad iraq tehran iran\n",
      "baghdad iraq athens greece\n",
      "beijing china canberra australia\n",
      "beijing china stockholm sweden\n",
      "beijing china tehran iran\n",
      "beijing china athens greece\n",
      "beijing china baghdad iraq\n",
      "canberra australia stockholm sweden\n",
      "canberra australia tehran iran\n",
      "canberra australia athens greece\n",
      "canberra australia baghdad iraq\n",
      "canberra australia beijing china\n",
      "stockholm sweden tehran iran\n",
      "stockholm sweden athens greece\n",
      "stockholm sweden baghdad iraq\n",
      "stockholm sweden beijing china\n",
      "stockholm sweden canberra australia\n",
      "tehran iran athens greece\n",
      "tehran iran baghdad iraq\n",
      "tehran iran beijing china\n",
      "tehran iran canberra australia\n",
      "tehran iran stockholm sweden\n",
      "accra ghana algiers algeria\n",
      "accra ghana amman jordan\n",
      "accra ghana apia samoa\n",
      "accra ghana athens greece\n",
      "accra ghana baghdad iraq\n",
      "accra ghana beijing china\n",
      "accra ghana belgrade serbia\n",
      "accra ghana canberra australia\n",
      "accra ghana copenhagen denmark\n",
      "accra ghana damascus syria\n",
      "accra ghana doha qatar\n",
      "algiers algeria amman jordan\n",
      "algiers algeria apia samoa\n",
      "algiers algeria athens greece\n",
      "algiers algeria baghdad iraq\n",
      "algiers algeria beijing china\n",
      "algiers algeria belgrade serbia\n",
      "algiers algeria canberra australia\n",
      "algiers algeria copenhagen denmark\n",
      "algiers algeria damascus syria\n",
      "algiers algeria doha qatar\n",
      "amman jordan apia samoa\n",
      "amman jordan athens greece\n",
      "amman jordan baghdad iraq\n",
      "amman jordan beijing china\n",
      "amman jordan belgrade serbia\n",
      "amman jordan canberra australia\n",
      "amman jordan copenhagen denmark\n",
      "amman jordan damascus syria\n",
      "amman jordan doha qatar\n",
      "apia samoa athens greece\n",
      "apia samoa baghdad iraq\n",
      "apia samoa beijing china\n",
      "apia samoa belgrade serbia\n",
      "apia samoa canberra australia\n",
      "apia samoa copenhagen denmark\n",
      "apia samoa damascus syria\n",
      "apia samoa doha qatar\n",
      "apia samoa harare zimbabwe\n",
      "athens greece baghdad iraq\n",
      "athens greece beijing china\n",
      "athens greece belgrade serbia\n",
      "athens greece canberra australia\n",
      "athens greece copenhagen denmark\n",
      "athens greece damascus syria\n",
      "athens greece doha qatar\n",
      "athens greece harare zimbabwe\n",
      "baghdad iraq beijing china\n",
      "baghdad iraq belgrade serbia\n",
      "baghdad iraq canberra australia\n",
      "baghdad iraq copenhagen denmark\n",
      "baghdad iraq damascus syria\n",
      "baghdad iraq doha qatar\n",
      "baghdad iraq harare zimbabwe\n",
      "beijing china belgrade serbia\n",
      "beijing china canberra australia\n",
      "beijing china copenhagen denmark\n",
      "beijing china damascus syria\n",
      "beijing china doha qatar\n",
      "beijing china harare zimbabwe\n",
      "beijing china kathmandu nepal\n",
      "beijing china kiev ukraine\n",
      "beijing china kingston jamaica\n",
      "belgrade serbia canberra australia\n",
      "belgrade serbia copenhagen denmark\n",
      "belgrade serbia damascus syria\n",
      "belgrade serbia doha qatar\n",
      "belgrade serbia harare zimbabwe\n",
      "belgrade serbia kathmandu nepal\n",
      "belgrade serbia kiev ukraine\n",
      "belgrade serbia kingston jamaica\n",
      "belgrade serbia libreville gabon\n",
      "canberra australia copenhagen denmark\n",
      "canberra australia damascus syria\n",
      "canberra australia doha qatar\n",
      "canberra australia harare zimbabwe\n",
      "canberra australia kathmandu nepal\n",
      "canberra australia kiev ukraine\n",
      "canberra australia kingston jamaica\n",
      "canberra australia libreville gabon\n",
      "canberra australia lisbon portugal\n",
      "canberra australia lusaka zambia\n",
      "copenhagen denmark damascus syria\n",
      "copenhagen denmark doha qatar\n",
      "copenhagen denmark harare zimbabwe\n",
      "copenhagen denmark kathmandu nepal\n",
      "copenhagen denmark kiev ukraine\n",
      "copenhagen denmark kingston jamaica\n",
      "copenhagen denmark libreville gabon\n",
      "copenhagen denmark lisbon portugal\n",
      "copenhagen denmark lusaka zambia\n",
      "copenhagen denmark mogadishu somalia\n",
      "damascus syria doha qatar\n",
      "damascus syria harare zimbabwe\n",
      "damascus syria kathmandu nepal\n",
      "damascus syria kiev ukraine\n",
      "damascus syria kingston jamaica\n",
      "damascus syria libreville gabon\n",
      "damascus syria lisbon portugal\n",
      "damascus syria lusaka zambia\n",
      "damascus syria mogadishu somalia\n",
      "doha qatar harare zimbabwe\n",
      "doha qatar kathmandu nepal\n",
      "doha qatar kiev ukraine\n",
      "doha qatar kingston jamaica\n",
      "doha qatar libreville gabon\n",
      "doha qatar lisbon portugal\n",
      "doha qatar lusaka zambia\n",
      "doha qatar mogadishu somalia\n",
      "doha qatar nairobi kenya\n",
      "harare zimbabwe kathmandu nepal\n",
      "harare zimbabwe kiev ukraine\n",
      "harare zimbabwe kingston jamaica\n",
      "harare zimbabwe libreville gabon\n",
      "harare zimbabwe lisbon portugal\n",
      "harare zimbabwe lusaka zambia\n",
      "harare zimbabwe mogadishu somalia\n",
      "harare zimbabwe nairobi kenya\n",
      "harare zimbabwe nuuk greenland\n",
      "harare zimbabwe paramaribo suriname\n",
      "kathmandu nepal kiev ukraine\n",
      "kathmandu nepal kingston jamaica\n",
      "kathmandu nepal libreville gabon\n",
      "kathmandu nepal lisbon portugal\n",
      "kathmandu nepal lusaka zambia\n",
      "kathmandu nepal mogadishu somalia\n",
      "kathmandu nepal nairobi kenya\n",
      "kathmandu nepal nuuk greenland\n",
      "kathmandu nepal paramaribo suriname\n",
      "kathmandu nepal rabat morocco\n",
      "kiev ukraine kingston jamaica\n",
      "kiev ukraine libreville gabon\n",
      "kiev ukraine lisbon portugal\n",
      "kiev ukraine lusaka zambia\n",
      "kiev ukraine mogadishu somalia\n",
      "kiev ukraine nairobi kenya\n",
      "kiev ukraine nuuk greenland\n",
      "kiev ukraine paramaribo suriname\n",
      "kiev ukraine rabat morocco\n",
      "kingston jamaica libreville gabon\n",
      "kingston jamaica lisbon portugal\n",
      "kingston jamaica lusaka zambia\n",
      "kingston jamaica mogadishu somalia\n",
      "kingston jamaica nairobi kenya\n",
      "kingston jamaica nuuk greenland\n",
      "kingston jamaica paramaribo suriname\n",
      "kingston jamaica rabat morocco\n",
      "kingston jamaica stockholm sweden\n",
      "libreville gabon lisbon portugal\n",
      "libreville gabon lusaka zambia\n",
      "libreville gabon mogadishu somalia\n",
      "libreville gabon nairobi kenya\n",
      "libreville gabon nuuk greenland\n",
      "libreville gabon paramaribo suriname\n",
      "libreville gabon rabat morocco\n",
      "libreville gabon stockholm sweden\n",
      "lisbon portugal lusaka zambia\n",
      "lisbon portugal mogadishu somalia\n",
      "lisbon portugal nairobi kenya\n",
      "lisbon portugal nuuk greenland\n",
      "lisbon portugal paramaribo suriname\n",
      "lisbon portugal rabat morocco\n",
      "lisbon portugal stockholm sweden\n",
      "lusaka zambia mogadishu somalia\n",
      "lusaka zambia nairobi kenya\n",
      "lusaka zambia nuuk greenland\n",
      "lusaka zambia paramaribo suriname\n",
      "lusaka zambia rabat morocco\n",
      "lusaka zambia stockholm sweden\n",
      "lusaka zambia tegucigalpa honduras\n",
      "lusaka zambia tehran iran\n",
      "mogadishu somalia nairobi kenya\n",
      "mogadishu somalia nuuk greenland\n",
      "mogadishu somalia paramaribo suriname\n",
      "mogadishu somalia rabat morocco\n",
      "mogadishu somalia stockholm sweden\n",
      "mogadishu somalia tegucigalpa honduras\n",
      "mogadishu somalia tehran iran\n",
      "mogadishu somalia tirana albania\n",
      "mogadishu somalia tripoli libya\n",
      "mogadishu somalia tunis tunisia\n",
      "mogadishu somalia vienna austria\n",
      "nairobi kenya nuuk greenland\n",
      "nairobi kenya paramaribo suriname\n",
      "nairobi kenya rabat morocco\n",
      "nairobi kenya stockholm sweden\n",
      "nairobi kenya tegucigalpa honduras\n",
      "nairobi kenya tehran iran\n",
      "nairobi kenya tirana albania\n",
      "nairobi kenya tripoli libya\n",
      "nairobi kenya tunis tunisia\n",
      "nairobi kenya vienna austria\n",
      "nairobi kenya vientiane laos\n",
      "nuuk greenland paramaribo suriname\n",
      "nuuk greenland rabat morocco\n",
      "nuuk greenland stockholm sweden\n",
      "nuuk greenland tegucigalpa honduras\n",
      "nuuk greenland tehran iran\n",
      "nuuk greenland tirana albania\n",
      "nuuk greenland tripoli libya\n",
      "nuuk greenland tunis tunisia\n",
      "nuuk greenland vienna austria\n",
      "nuuk greenland vientiane laos\n",
      "nuuk greenland accra ghana\n",
      "nuuk greenland algiers algeria\n",
      "nuuk greenland amman jordan\n",
      "paramaribo suriname rabat morocco\n",
      "paramaribo suriname stockholm sweden\n",
      "paramaribo suriname tegucigalpa honduras\n",
      "paramaribo suriname tehran iran\n",
      "paramaribo suriname tirana albania\n",
      "paramaribo suriname tripoli libya\n",
      "paramaribo suriname tunis tunisia\n",
      "paramaribo suriname vienna austria\n",
      "paramaribo suriname vientiane laos\n",
      "paramaribo suriname accra ghana\n",
      "paramaribo suriname algiers algeria\n",
      "paramaribo suriname amman jordan\n",
      "paramaribo suriname apia samoa\n",
      "rabat morocco stockholm sweden\n",
      "rabat morocco tegucigalpa honduras\n",
      "rabat morocco tehran iran\n",
      "rabat morocco tirana albania\n",
      "rabat morocco tripoli libya\n",
      "rabat morocco tunis tunisia\n",
      "rabat morocco vienna austria\n",
      "rabat morocco vientiane laos\n",
      "rabat morocco accra ghana\n",
      "rabat morocco algiers algeria\n",
      "rabat morocco amman jordan\n",
      "rabat morocco apia samoa\n",
      "rabat morocco athens greece\n",
      "stockholm sweden tegucigalpa honduras\n",
      "stockholm sweden tehran iran\n",
      "stockholm sweden tirana albania\n",
      "stockholm sweden tripoli libya\n",
      "stockholm sweden tunis tunisia\n",
      "stockholm sweden vienna austria\n",
      "stockholm sweden vientiane laos\n",
      "stockholm sweden accra ghana\n",
      "stockholm sweden algiers algeria\n",
      "stockholm sweden amman jordan\n",
      "stockholm sweden apia samoa\n",
      "stockholm sweden athens greece\n",
      "stockholm sweden baghdad iraq\n",
      "stockholm sweden beijing china\n",
      "tegucigalpa honduras tehran iran\n",
      "tegucigalpa honduras tirana albania\n",
      "tegucigalpa honduras tripoli libya\n",
      "tegucigalpa honduras tunis tunisia\n",
      "tegucigalpa honduras vienna austria\n",
      "tegucigalpa honduras vientiane laos\n",
      "tegucigalpa honduras accra ghana\n",
      "tegucigalpa honduras algiers algeria\n",
      "tegucigalpa honduras amman jordan\n",
      "tegucigalpa honduras apia samoa\n",
      "tegucigalpa honduras athens greece\n",
      "tegucigalpa honduras baghdad iraq\n",
      "tegucigalpa honduras beijing china\n",
      "tegucigalpa honduras belgrade serbia\n",
      "tehran iran tirana albania\n",
      "tehran iran tripoli libya\n",
      "tehran iran tunis tunisia\n",
      "tehran iran vienna austria\n",
      "tehran iran vientiane laos\n",
      "tehran iran accra ghana\n",
      "tehran iran algiers algeria\n",
      "tehran iran amman jordan\n",
      "tehran iran apia samoa\n",
      "tehran iran athens greece\n",
      "tehran iran baghdad iraq\n",
      "tehran iran beijing china\n",
      "tehran iran belgrade serbia\n",
      "tirana albania tripoli libya\n",
      "tirana albania tunis tunisia\n",
      "tirana albania vienna austria\n",
      "tirana albania vientiane laos\n",
      "tirana albania accra ghana\n",
      "tirana albania algiers algeria\n",
      "tirana albania amman jordan\n",
      "tirana albania apia samoa\n",
      "tirana albania athens greece\n",
      "tirana albania baghdad iraq\n",
      "tirana albania beijing china\n",
      "tirana albania belgrade serbia\n",
      "tripoli libya tunis tunisia\n",
      "tripoli libya vienna austria\n",
      "tripoli libya vientiane laos\n",
      "tripoli libya accra ghana\n",
      "tripoli libya algiers algeria\n",
      "tripoli libya amman jordan\n",
      "tripoli libya apia samoa\n",
      "tripoli libya athens greece\n",
      "tripoli libya baghdad iraq\n",
      "tripoli libya beijing china\n",
      "tripoli libya belgrade serbia\n",
      "tunis tunisia vienna austria\n",
      "tunis tunisia vientiane laos\n",
      "tunis tunisia accra ghana\n",
      "tunis tunisia algiers algeria\n",
      "tunis tunisia amman jordan\n",
      "tunis tunisia apia samoa\n",
      "tunis tunisia athens greece\n",
      "tunis tunisia baghdad iraq\n",
      "tunis tunisia beijing china\n",
      "tunis tunisia belgrade serbia\n",
      "tunis tunisia canberra australia\n",
      "vienna austria vientiane laos\n",
      "vienna austria accra ghana\n",
      "vienna austria algiers algeria\n",
      "vienna austria amman jordan\n",
      "vienna austria apia samoa\n",
      "vienna austria athens greece\n",
      "vienna austria baghdad iraq\n",
      "vienna austria beijing china\n",
      "vienna austria belgrade serbia\n",
      "vienna austria canberra australia\n",
      "vientiane laos accra ghana\n",
      "vientiane laos algiers algeria\n",
      "vientiane laos amman jordan\n",
      "vientiane laos apia samoa\n",
      "vientiane laos athens greece\n",
      "vientiane laos baghdad iraq\n",
      "vientiane laos beijing china\n",
      "vientiane laos belgrade serbia\n",
      "vientiane laos canberra australia\n",
      "vientiane laos copenhagen denmark\n",
      "brazil real canada dollar\n",
      "brazil real latvia lats\n",
      "brazil real romania leu\n",
      "canada dollar latvia lats\n",
      "canada dollar romania leu\n",
      "canada dollar brazil real\n",
      "latvia lats romania leu\n",
      "latvia lats brazil real\n",
      "latvia lats canada dollar\n",
      "romania leu brazil real\n",
      "romania leu canada dollar\n",
      "romania leu latvia lats\n",
      "phoenix arizona denver colorado\n",
      "phoenix arizona minneapolis minnesota\n",
      "denver colorado minneapolis minnesota\n",
      "denver colorado laredo texas\n",
      "denver colorado hialeah florida\n",
      "minneapolis minnesota laredo texas\n",
      "minneapolis minnesota hialeah florida\n",
      "minneapolis minnesota fontana california\n",
      "laredo texas hialeah florida\n",
      "laredo texas fontana california\n",
      "laredo texas phoenix arizona\n",
      "laredo texas denver colorado\n",
      "hialeah florida fontana california\n",
      "hialeah florida phoenix arizona\n",
      "hialeah florida denver colorado\n",
      "fontana california phoenix arizona\n",
      "fontana california denver colorado\n",
      "fontana california minneapolis minnesota\n",
      "father mother grandfather grandmother\n",
      "father mother grandson granddaughter\n",
      "father mother man woman\n",
      "father mother sons daughters\n",
      "father mother uncle aunt\n",
      "grandfather grandmother grandson granddaughter\n",
      "grandfather grandmother man woman\n",
      "grandfather grandmother sons daughters\n",
      "grandfather grandmother uncle aunt\n",
      "grandfather grandmother father mother\n",
      "grandson granddaughter man woman\n",
      "grandson granddaughter sons daughters\n",
      "grandson granddaughter uncle aunt\n",
      "grandson granddaughter father mother\n",
      "grandson granddaughter grandfather grandmother\n",
      "man woman sons daughters\n",
      "man woman uncle aunt\n",
      "man woman father mother\n",
      "man woman grandfather grandmother\n",
      "man woman grandson granddaughter\n",
      "sons daughters uncle aunt\n",
      "sons daughters father mother\n",
      "sons daughters grandfather grandmother\n",
      "sons daughters grandson granddaughter\n",
      "sons daughters man woman\n",
      "uncle aunt father mother\n",
      "uncle aunt grandfather grandmother\n",
      "uncle aunt grandson granddaughter\n",
      "uncle aunt man woman\n",
      "uncle aunt sons daughters\n",
      "amazing amazingly apparent apparently\n",
      "amazing amazingly efficient efficiently\n",
      "amazing amazingly happy happily\n",
      "amazing amazingly occasional occasionally\n",
      "amazing amazingly professional professionally\n",
      "amazing amazingly quiet quietly\n",
      "amazing amazingly rapid rapidly\n",
      "amazing amazingly reluctant reluctantly\n",
      "amazing amazingly serious seriously\n",
      "amazing amazingly swift swiftly\n",
      "amazing amazingly typical typically\n",
      "apparent apparently efficient efficiently\n",
      "apparent apparently happy happily\n",
      "apparent apparently occasional occasionally\n",
      "apparent apparently professional professionally\n",
      "apparent apparently quiet quietly\n",
      "apparent apparently rapid rapidly\n",
      "apparent apparently reluctant reluctantly\n",
      "apparent apparently serious seriously\n",
      "apparent apparently swift swiftly\n",
      "apparent apparently typical typically\n",
      "apparent apparently amazing amazingly\n",
      "efficient efficiently happy happily\n",
      "efficient efficiently occasional occasionally\n",
      "efficient efficiently professional professionally\n",
      "efficient efficiently quiet quietly\n",
      "efficient efficiently rapid rapidly\n",
      "efficient efficiently reluctant reluctantly\n",
      "efficient efficiently serious seriously\n",
      "efficient efficiently swift swiftly\n",
      "efficient efficiently typical typically\n",
      "efficient efficiently amazing amazingly\n",
      "efficient efficiently apparent apparently\n",
      "happy happily occasional occasionally\n",
      "happy happily professional professionally\n",
      "happy happily quiet quietly\n",
      "happy happily rapid rapidly\n",
      "happy happily reluctant reluctantly\n",
      "happy happily serious seriously\n",
      "happy happily swift swiftly\n",
      "happy happily typical typically\n",
      "happy happily amazing amazingly\n",
      "happy happily apparent apparently\n",
      "happy happily efficient efficiently\n",
      "occasional occasionally professional professionally\n",
      "occasional occasionally quiet quietly\n",
      "occasional occasionally rapid rapidly\n",
      "occasional occasionally reluctant reluctantly\n",
      "occasional occasionally serious seriously\n",
      "occasional occasionally swift swiftly\n",
      "occasional occasionally typical typically\n",
      "occasional occasionally amazing amazingly\n",
      "occasional occasionally apparent apparently\n",
      "occasional occasionally efficient efficiently\n",
      "occasional occasionally happy happily\n",
      "professional professionally quiet quietly\n",
      "professional professionally rapid rapidly\n",
      "professional professionally reluctant reluctantly\n",
      "professional professionally serious seriously\n",
      "professional professionally swift swiftly\n",
      "professional professionally typical typically\n",
      "professional professionally amazing amazingly\n",
      "professional professionally apparent apparently\n",
      "professional professionally efficient efficiently\n",
      "professional professionally happy happily\n",
      "professional professionally occasional occasionally\n",
      "quiet quietly rapid rapidly\n",
      "quiet quietly reluctant reluctantly\n",
      "quiet quietly serious seriously\n",
      "quiet quietly swift swiftly\n",
      "quiet quietly typical typically\n",
      "quiet quietly amazing amazingly\n",
      "quiet quietly apparent apparently\n",
      "quiet quietly efficient efficiently\n",
      "quiet quietly happy happily\n",
      "quiet quietly occasional occasionally\n",
      "quiet quietly professional professionally\n",
      "rapid rapidly reluctant reluctantly\n",
      "rapid rapidly serious seriously\n",
      "rapid rapidly swift swiftly\n",
      "rapid rapidly typical typically\n",
      "rapid rapidly amazing amazingly\n",
      "rapid rapidly apparent apparently\n",
      "rapid rapidly efficient efficiently\n",
      "rapid rapidly happy happily\n",
      "rapid rapidly occasional occasionally\n",
      "rapid rapidly professional professionally\n",
      "rapid rapidly quiet quietly\n",
      "reluctant reluctantly serious seriously\n",
      "reluctant reluctantly swift swiftly\n",
      "reluctant reluctantly typical typically\n",
      "reluctant reluctantly amazing amazingly\n",
      "reluctant reluctantly apparent apparently\n",
      "reluctant reluctantly efficient efficiently\n",
      "reluctant reluctantly happy happily\n",
      "reluctant reluctantly occasional occasionally\n",
      "reluctant reluctantly professional professionally\n",
      "reluctant reluctantly quiet quietly\n",
      "reluctant reluctantly rapid rapidly\n",
      "serious seriously swift swiftly\n",
      "serious seriously typical typically\n",
      "serious seriously amazing amazingly\n",
      "serious seriously apparent apparently\n",
      "serious seriously efficient efficiently\n",
      "serious seriously happy happily\n",
      "serious seriously occasional occasionally\n",
      "serious seriously professional professionally\n",
      "serious seriously quiet quietly\n",
      "serious seriously rapid rapidly\n",
      "serious seriously reluctant reluctantly\n",
      "swift swiftly typical typically\n",
      "swift swiftly amazing amazingly\n",
      "swift swiftly apparent apparently\n",
      "swift swiftly efficient efficiently\n",
      "swift swiftly happy happily\n",
      "swift swiftly occasional occasionally\n",
      "swift swiftly professional professionally\n",
      "swift swiftly quiet quietly\n",
      "swift swiftly rapid rapidly\n",
      "swift swiftly reluctant reluctantly\n",
      "swift swiftly serious seriously\n",
      "typical typically amazing amazingly\n",
      "typical typically apparent apparently\n",
      "typical typically efficient efficiently\n",
      "typical typically happy happily\n",
      "typical typically occasional occasionally\n",
      "typical typically professional professionally\n",
      "typical typically quiet quietly\n",
      "typical typically rapid rapidly\n",
      "typical typically reluctant reluctantly\n",
      "typical typically serious seriously\n",
      "typical typically swift swiftly\n",
      "certain uncertain convenient inconvenient\n",
      "certain uncertain informed uninformed\n",
      "certain uncertain possible impossible\n",
      "certain uncertain responsible irresponsible\n",
      "certain uncertain tasteful distasteful\n",
      "convenient inconvenient informed uninformed\n",
      "convenient inconvenient possible impossible\n",
      "convenient inconvenient responsible irresponsible\n",
      "convenient inconvenient tasteful distasteful\n",
      "convenient inconvenient certain uncertain\n",
      "informed uninformed possible impossible\n",
      "informed uninformed responsible irresponsible\n",
      "informed uninformed tasteful distasteful\n",
      "informed uninformed certain uncertain\n",
      "informed uninformed convenient inconvenient\n",
      "possible impossible responsible irresponsible\n",
      "possible impossible tasteful distasteful\n",
      "possible impossible certain uncertain\n",
      "possible impossible convenient inconvenient\n",
      "possible impossible informed uninformed\n",
      "responsible irresponsible tasteful distasteful\n",
      "responsible irresponsible certain uncertain\n",
      "responsible irresponsible convenient inconvenient\n",
      "responsible irresponsible informed uninformed\n",
      "responsible irresponsible possible impossible\n",
      "tasteful distasteful certain uncertain\n",
      "tasteful distasteful convenient inconvenient\n",
      "tasteful distasteful informed uninformed\n",
      "tasteful distasteful possible impossible\n",
      "tasteful distasteful responsible irresponsible\n",
      "bright brighter cheap cheaper\n",
      "bright brighter easy easier\n",
      "bright brighter heavy heavier\n",
      "bright brighter old older\n",
      "bright brighter weak weaker\n",
      "cheap cheaper easy easier\n",
      "cheap cheaper heavy heavier\n",
      "cheap cheaper old older\n",
      "cheap cheaper weak weaker\n",
      "cheap cheaper bright brighter\n",
      "easy easier heavy heavier\n",
      "easy easier old older\n",
      "easy easier weak weaker\n",
      "easy easier bright brighter\n",
      "easy easier cheap cheaper\n",
      "heavy heavier old older\n",
      "heavy heavier weak weaker\n",
      "heavy heavier bright brighter\n",
      "heavy heavier cheap cheaper\n",
      "heavy heavier easy easier\n",
      "old older weak weaker\n",
      "old older bright brighter\n",
      "old older cheap cheaper\n",
      "old older easy easier\n",
      "old older heavy heavier\n",
      "weak weaker bright brighter\n",
      "weak weaker cheap cheaper\n",
      "weak weaker easy easier\n",
      "weak weaker heavy heavier\n",
      "weak weaker old older\n",
      "big biggest cold coldest\n",
      "cold coldest big biggest\n",
      "cambodia cambodian france french\n",
      "cambodia cambodian peru peruvian\n",
      "cambodia cambodian russia russian\n",
      "france french peru peruvian\n",
      "france french russia russian\n",
      "france french cambodia cambodian\n",
      "peru peruvian russia russian\n",
      "peru peruvian cambodia cambodian\n",
      "peru peruvian france french\n",
      "russia russian cambodia cambodian\n",
      "russia russian france french\n",
      "russia russian peru peruvian\n",
      "decreasing decreased enhancing enhanced\n",
      "decreasing decreased flying flew\n",
      "decreasing decreased hiding hid\n",
      "decreasing decreased hitting hit\n",
      "decreasing decreased selling sold\n",
      "decreasing decreased sitting sat\n",
      "decreasing decreased slowing slowed\n",
      "decreasing decreased walking walked\n",
      "decreasing decreased writing wrote\n",
      "enhancing enhanced flying flew\n",
      "enhancing enhanced hiding hid\n",
      "enhancing enhanced hitting hit\n",
      "enhancing enhanced selling sold\n",
      "enhancing enhanced sitting sat\n",
      "enhancing enhanced slowing slowed\n",
      "enhancing enhanced walking walked\n",
      "enhancing enhanced writing wrote\n",
      "enhancing enhanced decreasing decreased\n",
      "flying flew hiding hid\n",
      "flying flew hitting hit\n",
      "flying flew selling sold\n",
      "flying flew sitting sat\n",
      "flying flew slowing slowed\n",
      "flying flew walking walked\n",
      "flying flew writing wrote\n",
      "flying flew decreasing decreased\n",
      "flying flew enhancing enhanced\n",
      "hiding hid hitting hit\n",
      "hiding hid selling sold\n",
      "hiding hid sitting sat\n",
      "hiding hid slowing slowed\n",
      "hiding hid walking walked\n",
      "hiding hid writing wrote\n",
      "hiding hid decreasing decreased\n",
      "hiding hid enhancing enhanced\n",
      "hiding hid flying flew\n",
      "hitting hit selling sold\n",
      "hitting hit sitting sat\n",
      "hitting hit slowing slowed\n",
      "hitting hit walking walked\n",
      "hitting hit writing wrote\n",
      "hitting hit decreasing decreased\n",
      "hitting hit enhancing enhanced\n",
      "hitting hit flying flew\n",
      "hitting hit hiding hid\n",
      "selling sold sitting sat\n",
      "selling sold slowing slowed\n",
      "selling sold walking walked\n",
      "selling sold writing wrote\n",
      "selling sold decreasing decreased\n",
      "selling sold enhancing enhanced\n",
      "selling sold flying flew\n",
      "selling sold hiding hid\n",
      "selling sold hitting hit\n",
      "sitting sat slowing slowed\n",
      "sitting sat walking walked\n",
      "sitting sat writing wrote\n",
      "sitting sat decreasing decreased\n",
      "sitting sat enhancing enhanced\n",
      "sitting sat flying flew\n",
      "sitting sat hiding hid\n",
      "sitting sat hitting hit\n",
      "sitting sat selling sold\n",
      "slowing slowed walking walked\n",
      "slowing slowed writing wrote\n",
      "slowing slowed decreasing decreased\n",
      "slowing slowed enhancing enhanced\n",
      "slowing slowed flying flew\n",
      "slowing slowed hiding hid\n",
      "slowing slowed hitting hit\n",
      "slowing slowed selling sold\n",
      "slowing slowed sitting sat\n",
      "walking walked writing wrote\n",
      "walking walked decreasing decreased\n",
      "walking walked enhancing enhanced\n",
      "walking walked flying flew\n",
      "walking walked hiding hid\n",
      "walking walked hitting hit\n",
      "walking walked selling sold\n",
      "walking walked sitting sat\n",
      "walking walked slowing slowed\n",
      "writing wrote decreasing decreased\n",
      "writing wrote enhancing enhanced\n",
      "writing wrote flying flew\n",
      "writing wrote hiding hid\n",
      "writing wrote hitting hit\n",
      "writing wrote selling sold\n",
      "writing wrote sitting sat\n",
      "writing wrote slowing slowed\n",
      "writing wrote walking walked\n",
      "elephant elephants eye eyes\n",
      "elephant elephants horse horses\n",
      "elephant elephants lion lions\n",
      "elephant elephants mango mangoes\n",
      "elephant elephants road roads\n",
      "eye eyes horse horses\n",
      "eye eyes lion lions\n",
      "eye eyes mango mangoes\n",
      "eye eyes road roads\n",
      "eye eyes elephant elephants\n",
      "horse horses lion lions\n",
      "horse horses mango mangoes\n",
      "horse horses road roads\n",
      "horse horses elephant elephants\n",
      "horse horses eye eyes\n",
      "lion lions mango mangoes\n",
      "lion lions road roads\n",
      "lion lions elephant elephants\n",
      "lion lions eye eyes\n",
      "lion lions horse horses\n",
      "mango mangoes road roads\n",
      "mango mangoes elephant elephants\n",
      "mango mangoes eye eyes\n",
      "mango mangoes horse horses\n",
      "mango mangoes lion lions\n",
      "road roads elephant elephants\n",
      "road roads eye eyes\n",
      "road roads horse horses\n",
      "road roads lion lions\n",
      "road roads mango mangoes\n",
      "increase increases listen listens\n",
      "increase increases provide provides\n",
      "increase increases scream screams\n",
      "increase increases search searches\n",
      "increase increases talk talks\n",
      "listen listens provide provides\n",
      "listen listens scream screams\n",
      "listen listens search searches\n",
      "listen listens talk talks\n",
      "listen listens increase increases\n",
      "provide provides scream screams\n",
      "provide provides search searches\n",
      "provide provides talk talks\n",
      "provide provides increase increases\n",
      "provide provides listen listens\n",
      "scream screams search searches\n",
      "scream screams talk talks\n",
      "scream screams increase increases\n",
      "scream screams listen listens\n",
      "scream screams provide provides\n",
      "search searches talk talks\n",
      "search searches increase increases\n",
      "search searches listen listens\n",
      "search searches provide provides\n",
      "search searches scream screams\n",
      "talk talks increase increases\n",
      "talk talks listen listens\n",
      "talk talks provide provides\n",
      "talk talks scream screams\n",
      "talk talks search searches\n"
     ]
    }
   ],
   "source": [
    "# Initiialize\n",
    "questions, pairs = sample_questions(filename, n_pairs)\n",
    "int_to_word, word_to_int = initialize_dict(questions)\n",
    "questions = question_to_int(questions, word_to_int)\n",
    "n_question = len(questions)\n",
    "\n",
    "# Change new parameters\n",
    "n_word = len(int_to_word)\n",
    "output_dict = 'output/{}-context-{}-data-{}-questions/'.format(n_context, n_data, n_question)\n",
    "\n",
    "# Initialize dummy contexts\n",
    "int_to_cont = {context: context for context in range(n_context)}\n",
    "cont_to_int = {word: ii for ii, word in int_to_cont.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample context distribution for each words\n",
    "original_distribution, word_dis_dict = generate_original_distribution(questions, n_context)\n",
    "noise_distribution = generate_noise_distribution(n_pairs, n_context)\n",
    "context_distribution = {}\n",
    "\n",
    "for pair, idx in pairs.items():\n",
    "    word1, word2 = pair\n",
    "    word1 = word_to_int[word1]\n",
    "    word2 = word_to_int[word2]\n",
    "    \n",
    "    # word1\n",
    "    dis = (1- noise_rate) * original_distribution[word_dis_dict[word1]].copy() + noise_rate * noise_distribution[idx]\n",
    "    dis = dis / np.sum(dis)\n",
    "    context_distribution[word1] = dis\n",
    "    \n",
    "    # word2\n",
    "    dis = (1- noise_rate) * original_distribution[word_dis_dict[word2]].copy() + noise_rate * noise_distribution[idx]\n",
    "    dis = dis / np.sum(dis)\n",
    "    context_distribution[word2] = dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1 manila-philippines: 0.7654407703943393, Pair 2 ottawa-canada: 0.7654407703943393, diff: 0.0\n",
      "Pair 1 invent-inventing: 0.7768114965702747, Pair 2 listen-listening: 0.7768114965702747, diff: 0.0\n",
      "Pair 1 listen-listening: 0.7768114965702747, Pair 2 invent-inventing: 0.7768114965702747, diff: 0.0\n",
      "Pair 1 israel-israeli: 0.7524785736928562, Pair 2 peru-peruvian: 0.7524785736928562, diff: 0.0\n",
      "Pair 1 peru-peruvian: 0.7524785736928562, Pair 2 israel-israeli: 0.7524785736928562, diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test for word analogy property\n",
    "for i in range(n_question):\n",
    "    question = questions[i]\n",
    "    pair1_dis = cosine(context_distribution[question[0]], context_distribution[question[1]])\n",
    "    pair2_dis = cosine(context_distribution[question[2]], context_distribution[question[3]])\n",
    "    print('Pair 1 {}-{}: {}, Pair 2 {}-{}: {}, diff: {}'.format(int_to_word[question[0]], int_to_word[question[1]], pair1_dis,\n",
    "                                                                int_to_word[question[2]], int_to_word[question[3]], pair2_dis,\n",
    "                                                                abs(pair1_dis - pair2_dis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1 manila-listening: 0.7468246958433704, Pair 2 listening-peru: 0.836128336664007, diff: 0.08930364082063658\n"
     ]
    }
   ],
   "source": [
    "# Random pairs\n",
    "pair1_dis = cosine(context_distribution[questions[0][0]], context_distribution[questions[1][3]])\n",
    "pair2_dis = cosine(context_distribution[questions[2][1]], context_distribution[questions[3][2]])\n",
    "print('Pair 1 {}-{}: {}, Pair 2 {}-{}: {}, diff: {}'.format(int_to_word[questions[0][0]], int_to_word[questions[1][3]], pair1_dis,\n",
    "                                                            int_to_word[questions[2][1]], int_to_word[questions[3][2]], pair2_dis,\n",
    "                                                            abs(pair1_dis - pair2_dis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82236467 0.         0.76675514 0.         0.82236467 0.76780085\n",
      " 0.76675514 1.         0.76780085 0.         0.74120291 0.74120291]\n",
      "[0.83612834 0.7468247  0.         0.7468247  0.83612834 0.\n",
      " 1.         0.76675514 0.         0.76675514 0.74363747 0.74363747]\n",
      "[0.83612834 0.7468247  1.         0.7468247  0.83612834 0.\n",
      " 0.         0.76675514 0.         0.76675514 0.74363747 0.74363747]\n",
      "[0.         0.77935171 0.74363747 0.77935171 0.         0.78587478\n",
      " 0.74363747 0.74120291 0.78587478 0.74120291 1.         0.        ]\n",
      "[0.         0.77935171 0.74363747 0.77935171 0.         0.78587478\n",
      " 0.74363747 0.74120291 0.78587478 0.74120291 0.         1.        ]\n",
      "Acc:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Test word analogy score\n",
    "result = []\n",
    "for question in questions:\n",
    "    # Predict\n",
    "    answer = question[3]\n",
    "    pred = context_distribution[question[1]] - context_distribution[question[0]] + context_distribution[question[2]]\n",
    "    sim_vector = np.zeros(n_word)\n",
    "    for i in range(n_word):\n",
    "        if i in question[:3]:\n",
    "            sim_vector[i] = 0\n",
    "        else:\n",
    "            sim_vector[i] = cosine(pred, context_distribution[i])\n",
    "    pred = np.argsort(sim_vector)[-1]\n",
    "    print(sim_vector)\n",
    "    result.append(1 if pred == answer else 0)\n",
    "\n",
    "print('Acc: ', np.mean(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training set\n",
    "data = []\n",
    "for i in range(n_data):\n",
    "    # Sample word\n",
    "    word = random.randrange(n_word)\n",
    "    \n",
    "    # Sample context\n",
    "    dis = context_distribution[word]\n",
    "    context = np.argmax(np.random.multinomial(1, dis))\n",
    "    \n",
    "    data.append([word, context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing processed data back to file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# make directories\n",
    "if not os.path.exists(output_dict):\n",
    "    os.makedirs(output_dict)\n",
    "\n",
    "# Save data\n",
    "print('Writing processed data back to file...')\n",
    "output = open(output_dict + 'data.csv', \"w\", newline='')\n",
    "writer = csv.writer(output)\n",
    "writer.writerows(data)\n",
    "output.close()\n",
    "\n",
    "# Save dictionaries\n",
    "save_pkl(int_to_word, output_dict + 'dict/int_to_vocab.dict')\n",
    "save_pkl(word_to_int, output_dict + 'dict/vocab_to_int.dict')\n",
    "save_pkl(cont_to_int, output_dict + 'dict/cont_to_int.dict')\n",
    "save_pkl(int_to_cont, output_dict + 'dict/int_to_cont.dict')\n",
    "print('Done!')\n",
    "\n",
    "# Save questions\n",
    "with open(filepath + 'test-{}-questions.txt'.format(n_question), 'w') as f:\n",
    "    f.write(': test-category\\n')\n",
    "    for i in range(n_question):\n",
    "        question = questions[i]\n",
    "        f.write('{} {} {} {}\\n'.format(int_to_word[question[0]], int_to_word[question[1]], int_to_word[question[2]], int_to_word[question[3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doha-qatar-jakarta-indonesia\n",
      "havana-cuba-kampala-uganda\n",
      "yerevan-armenia-copenhagen-denmark\n",
      "irvine-california-chicago-illinois\n",
      "happy-happily-obvious-obviously\n",
      "immediate-immediately-typical-typically\n",
      "obvious-obviously-happy-happily\n",
      "typical-typically-immediate-immediately\n",
      "loud-louder-low-lower\n",
      "low-lower-loud-louder\n",
      "norway-norwegian-sweden-swedish\n",
      "sweden-swedish-norway-norwegian\n",
      "banana-bananas-man-men\n",
      "cloud-clouds-computer-computers\n",
      "computer-computers-cloud-clouds\n",
      "man-men-banana-bananas\n"
     ]
    }
   ],
   "source": [
    "# Test for word analogy property\n",
    "for i in range(n_question):\n",
    "    question = questions[i]\n",
    "    print('{}-{}-{}-{}'.format(int_to_word[question[0]], int_to_word[question[1]], int_to_word[question[2]], int_to_word[question[3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
